\documentclass[11pt, a4paper, oneside]{book}
\usepackage{amsmath,amsthm,amssymb, enumitem, etoolbox, mathtools, centernot, microtype, geometry, tcolorbox,lipsum, makeidx, csquotes, lscape, graphicx, wrapfig, titling}
\usepackage[colorlinks,linkcolor=black,anchorcolor=black,
citecolor=green,CJKbookmarks=True]{hyperref}
\usepackage{fancyhdr}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=cyan,
}

\usepackage[font={it, small}]{caption}

\urlstyle{same}

\pagestyle{fancy}
\fancyhf{}
\chead{\emph{Probability Theory and Statistics Notebook I}}
\cfoot{\thepage}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem*{remark}{Remark}

\newcommand{\var}[1]{\text{Var}(#1)}
\newcommand{\cov}[1]{\text{Cov}(#1)}
\newcommand{\E}[1]{\text{E}[#1]}

\newcommand{\poisson}[1]{\text{Poisson}(#1)}
\newcommand{\gammaf}[1]{\Gamma(#1)}
\newcommand{\normal}[2]{\text{Normal}(#1, #2)}
\newcommand{\gammad}[2]{\text{Gamma}(#1, #2)}
\newcommand{\binomial}[2]{\text{Binom}(#1, #2)}
\newcommand{\markov}[2]{\text{Markov}(#1, #2)}
\newcommand{\Exp}[1]{\text{Exp}(#1)}


\def\R{\mathbb{R}}
\def\Z{\mathbb{Z}}
\def\cprocess{\{N(t), t\ge 0\}}
\def\lra{\leftrightarrow}

\newcommand{\breaking}{%
    \begin{center}
    $-$
    \end{center}%
}

\makeindex

\setlength{\parskip}{0.5em}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{1}



\begin{document}
\frontmatter 

\title{\huge Probability Theory and Statistics Notebook I}
\author{\Large{Zhang Ruiyang}}
\date{}
\maketitle

\tableofcontents

\newpage

\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface}

\mainmatter

\part{Probability Theory} 

\chapter{Fundamentals}
\chapter{Random Variables}
\chapter{Common Distributions}
\chapter{Limit Theorem}

\newpage


\chapter{Poisson Process}

\section{Introduction to Counting Process}

\noindent In the study of theory of probability, we do not only study individual objects, we also would index various things into categories and discuss their relationships with one another. That is known as \textbf{stochastic process}, which is a mathematical object usually defined as a family of random variables. 

\noindent We will be looking at two kinds of stochastic processes in this notebook. One is \textbf{counting process} that includes \textbf{Poisson process} and \textbf{Renewal Theory}. This will be elaborated in this Chapter. The other kind of stochastic process is \textbf{Markov chain} which will be studied in the next Chapter. 

\noindent Before learning about specific kinds of counting process, let us have a brief understanding of what a counting process is. 

\noindent A counting process\index{Counting Process} is a stochastic process $\cprocess$ with values that are nonnegative, integer and non-decreasing, or equivalently:

1. $N(t) \ge 0$.

2. $N(t)$ is an integer.

3. If $s \le t$, then $N(s) \le N(t)$.

\noindent Also, we should note that $t \in [0,\infty )$, which implies `time' is continuous here. 

\noindent If $N_t$ describes an arrival process, then $N_t = k$ means that there have been $k$ arrivals in the time interval $[0,t]$. In fact, we can describe the process by the sequence of arrival times, which we might call ``points" of the process. Let $T_k = \inf\{t\ge 0: N_t \ge k\}$ for $k \ge 0$. Then $T_0 = 0$ and $T_k$ is the ``$k$-th arrival time" for $k \ge 1$. We also define $Y_k = T_k - T_{k-1}$ for $k \ge 1$. Here the $Y_k$ is the ``inter-arrival time" between arrivals $k-1$ and $k$. 

\noindent In addition, for $s < t$, we write $N(s,t]$ for $N_t - N_s$, which we can think of as the number of points of the process which occur in the time-interval $(s,t]$. This is also called the ``increment" of the process $N$ on the interval $(s,t]$. 

\section{Homogeneous Poisson Process}

\noindent The name `Poisson' is used in the name of the process for a reason. The other thing that is named after the same person is the Poisson distribution, which is a discrete distribution modelling the occurrence of an unlikely event. The readers who have forgotten about it can refer to Chapter 3.3. The reason why this type of stochastic process is named after Poisson is because, although Poisson never actually worked on this at all during his life time, it follows the same Poisson distribution to some extent. We will see that connection more closely as we proceed with this topic. 

\noindent A stream of events occurring one by one in continuous time, in an uncoordinated way can be modelled by a \textbf{Poisson process}\index{Poisson Process}. For example, it can very accurately model the process of times of detections by a Geiger counter near a radioactive source, or it can be a good model for the process of times of arrivals of calls at a call centre. 

\noindent There are \textbf{two ways} to define a Poisson process. We will show both ways and later on explain the equivalent relationship of the two. 

\noindent The first definition uses exponential inter-arrival times of two consecutive occurrences. $\cprocess$ is a Poisson process of rate $\lambda$ if its inter-arrival times $Y_1, Y_2, \dots$ are i.i.d. with \underline{$\Exp{\lambda}$ distribution}. 

\noindent The second definition uses Poisson distribution for increments. $\cprocess$ is a Poisson process with rate $\lambda$ if 

1. $N(0) = 0$.

2. The process has independent increments.

3. The number of events in any increment of length $t$ is \underline{Poisson distributed} with mean $\lambda t$. That is, for all $s, t \ge 0$,\[
P\{N(t+s) - N(s) = n\} = e^{-\lambda t}\frac{(\lambda t)^n}{n!}
\]
where $n = 0, 1$.

\noindent The second point above uses the term \textbf{independent increment}. That means the numbers of events that occur in disjoint time intervals are independent. 

\breaking

\noindent Now, we want to show the equivalence. The main idea is to show that the memoryless property for the exponential distribution and the independent increments property are essentially the same. 

\noindent Property 1 of the second definition can be shown immediately as $Y_1 = T_1 = \inf\{t \ge 0: N_t \ge 1\}$ is strictly positive with probability 1, also $N_0 = 0$ with probability 1. 

\noindent Next, we should consider about the distribution of the number of points in an interval. Without loss of generality, we set $s = 0$ for property 3 and consider $N(0,t]$. We need to show $N(0,t] \sim \poisson{\lambda t}$, or for all $k$, \[
P(N(0,t] = k) = \frac{e^{-\lambda t}(\lambda t)^k}{k!}.
\]

\noindent By rewriting the LHS using $T_k$ and $T_{k+1}$, along with $T_k \sim \gammad{k}{\lambda}$ and $T_{k+1} \sim \gammad{k+1}{\lambda}$ since $T_k$ is the sum of $k$ i.i.d. exponential distribution of rate $\lambda$, we get
\begin{equation*}
\begin{split}
P(N(0,t] = k) &= P(T_k \le t, T_{k+1} > t) \\
&= P(T_k \le t) - P(T_{k+1} \le t) \\
&= \int_0^t \frac{e^{-\lambda x}{\lambda}^k x^{k-1}}{(k-1)!} dx - \int_0^t \frac{e^{-\lambda x}{\lambda}^{k+1} x^{k}}{k!} dx \\ 
&= \frac{e^{-\lambda t}(\lambda t)^k}{k!} \\ 
\end{split}
\end{equation*}
using integration by parts. 

\noindent Due to the memoryless property of exponential distribution and independence, a change in value of $s$ will not be affecting the above result. This means the second definition can be implied by the first one. The reverse statement can be shown by direct computation, which will be omitted here. 

\section{Superposition and Thinning} 

\noindent A Poisson process can be superposed, and it can also be thinned. 

\noindent For superposition, if we have two independent Poisson processes $L_t$ and $M_t$ with rate $\lambda$ and $\mu$ respectively, and let $N_t = L_t + M_t$, we get a Poisson process $N_t$ with rate $\lambda + \mu$. This can be shown by simple manipulations of the definitions, which will be omitted. 

\noindent For thinning, if we have a Poisson process $N_t$ with rate $\lambda$. We mark each point of the process with probability $p$ independently. We then let the counting process of the marked points be $M_t$, it is then a Poisson process of rate $p\lambda$. Proofs will be skipped. 

\noindent Their is a stronger statement of thinning. With the same setting, we can single out the unmarked points and let the counting process of those points be $L$, that is again a counting process of rate $(1-p)\lambda$ and it is independent of $M$. This is quite logical if we think about the superposition property of $N = M + L$. 

\noindent An application of the extended version of thinning can be used to study the queueing system M/G/1. 

\section{Nonhomogeneous Poisson Process}

\noindent So far, we have only discussed the case of constant rate, also known as the \textbf{homogeneous} case. The rate of the Poisson process can in fact be changing. In that case, the Poisson process will be \textbf{nonhomogeneous}\index{Poisson Process!Nonhomogeneous}, since they are not of the same `kind' any more. 

\noindent To better illustrate the case of nonhomogeneous Poisson process, we first introduce a third way to define a homogeneous Poisson process. 

\noindent Before going into the definition, we will need to define something else. A function $f$ is said to be $o(h)$ if $\lim_(h \to 0) \frac{f(h)}{h} = 0$. 

\noindent Now, for a counting process $\cprocess$, it is said to be a Poisson process with rate $\lambda$ if

1. $N(0) = 0$.

2. The process has stationary and independent increments.

3. $P\{N(h) = 1\} = \lambda h + o(h)$.

4. $P\{N(h) \ge 2\} = o(h)$.

\noindent Here, stationary increment means the distribution of the number of events that occur in any interval of time depends only on the length of the time interval. Additionally, just like the two ways before, this definition is equivalent to the other. 

\noindent Knowing those, we are ready to give the definition of nonhomogeneous Poisson process. 

\noindent For a counting process $\cprocess$, it is said to be a nonhomogeneous or nonstationary Poisson process with intensity function $\lambda(t)$ if

1. $N(0) = 0$.

2. The process has independent increments.

3. $P\{N(t+h) - N(t) = 1\} = \lambda(t) h + o(h)$.

4. $P\{N(t+h) - N(t) \ge 2\} = o(h)$.

\noindent If we let $m(t) = \int_0^t \lambda(s) ds$, we can show that \[
P\{ N(t+s) - N(t) = n\} = \exp\{ -(m(t+s)-m(t))\} [m(t+s)-m(t)]^n / n!
\]
which means $N(t+s) - N(t)$ is Poisson distributed with mean $m(t+s) - m(t)$. 

\noindent There are many applications and generalisations of Poisson process, such as renewal theory, queueing theory, spatial statistics and stochastic geometry. We will leave those extension for the readers to discover. Some of them will be studied under courses with the names along the line of `Applied Probability'. 


\backmatter


\end{document}
